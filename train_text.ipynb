{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff44ebf911b64b19975f1e9ac62c6f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cuda',\n",
       " PNDMScheduler {\n",
       "   \"_class_name\": \"PNDMScheduler\",\n",
       "   \"_diffusers_version\": \"0.27.0.dev0\",\n",
       "   \"beta_end\": 0.012,\n",
       "   \"beta_schedule\": \"scaled_linear\",\n",
       "   \"beta_start\": 0.00085,\n",
       "   \"clip_sample\": false,\n",
       "   \"num_train_timesteps\": 1000,\n",
       "   \"prediction_type\": \"epsilon\",\n",
       "   \"set_alpha_to_one\": false,\n",
       "   \"skip_prk_steps\": true,\n",
       "   \"steps_offset\": 1,\n",
       "   \"timestep_spacing\": \"leading\",\n",
       "   \"trained_betas\": null\n",
       " },\n",
       " CLIPTokenizer(name_or_path='C:\\Users\\37026\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9\\tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " })"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    'runwayml/stable-diffusion-v1-5', safety_checker=None)\n",
    "\n",
    "scheduler = pipeline.scheduler\n",
    "tokenizer = pipeline.tokenizer\n",
    "\n",
    "del pipeline\n",
    "\n",
    "device, scheduler, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file):\n",
    "    with open(file) as f:\n",
    "        text = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            #remove \\n\n",
    "            line = line.strip()\n",
    "            text.append(line)\n",
    "    return text\n",
    "\n",
    "def read_images(dir):\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    return [Image.open(os.path.join(dir, i)) for i in os.listdir(dir) if i.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\n",
    "    'a photo of a [snorlax_bear]', 'a rendering of a [snorlax_bear]',\n",
    "    'a dark photo of the [snorlax_bear]', 'a photo of my [snorlax_bear]',\n",
    "    'a good photo of a [snorlax_bear]', 'a photo of the nice [snorlax_bear]',\n",
    "    'a photo of the small [snorlax_bear]', 'a photo of the weird [snorlax_bear]',\n",
    "    'a photo of the cool [snorlax_bear]', 'a close-up photo of a [snorlax_bear]',\n",
    "    'a bright photo of the [snorlax_bear]', 'a cropped photo of a [snorlax_bear]',\n",
    "    'a cropped photo of the [snorlax_bear]', 'the photo of a [snorlax_bear]',\n",
    "    'a photo of a clean [snorlax_bear]', 'a photo of a dirty [snorlax_bear]',\n",
    "    'a photo of the large [snorlax_bear]', 'a photo of a cool [snorlax_bear]',\n",
    "    'a photo of a small [snorlax_bear]'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "def create_dataset(dir_1):\n",
    "\n",
    "    images = read_images(dir_1)\n",
    "    text = random.choices(texts, k=len(images))\n",
    " \n",
    "    # Create a Hugging Face dataset\n",
    "    dataset = Dataset.from_dict({'image': images, 'text': text})\n",
    "\n",
    "    return dataset\n",
    "pika= create_dataset('./Snorlax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc12a1b246df4290b167c1231960eb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision\n",
    "\n",
    "#图像增强模块\n",
    "compose = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Lambda(lambda img: img.convert('RGB')), \n",
    "    torchvision.transforms.Resize(\n",
    "        512, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    torchvision.transforms.CenterCrop(512),\n",
    "    #torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "    \n",
    "def f(data):\n",
    "    #image enhance\n",
    "    pixel_values = [compose(i) for i in data['image']]\n",
    "\n",
    "    #text encode\n",
    "    input_ids = tokenizer.batch_encode_plus(data['text'],\n",
    "                                            padding='max_length',\n",
    "                                            truncation=True,\n",
    "                                            max_length=77).input_ids\n",
    "    \n",
    "\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "dataset = pika.map(f,\n",
    "                      batched=True,\n",
    "                      batch_size=100,\n",
    "                      num_proc=1,\n",
    "                      remove_columns=['image', 'text' ])\n",
    "\n",
    "dataset.set_format(type='torch')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " {'pixel_values': tensor([[[[0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "            [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "            [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "            ...,\n",
       "            [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "            [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "            [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216]],\n",
       "  \n",
       "           [[0.9765, 0.9765, 0.9765,  ..., 0.9765, 0.9765, 0.9765],\n",
       "            [0.9765, 0.9765, 0.9765,  ..., 0.9765, 0.9765, 0.9765],\n",
       "            [0.9765, 0.9765, 0.9765,  ..., 0.9765, 0.9765, 0.9765],\n",
       "            ...,\n",
       "            [0.9765, 0.9765, 0.9765,  ..., 0.9765, 0.9765, 0.9765],\n",
       "            [0.9765, 0.9765, 0.9765,  ..., 0.9765, 0.9765, 0.9765],\n",
       "            [0.9765, 0.9765, 0.9765,  ..., 0.9765, 0.9765, 0.9765]],\n",
       "  \n",
       "           [[0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n",
       "            [0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n",
       "            [0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n",
       "            ...,\n",
       "            [0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n",
       "            [0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n",
       "            [0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137]]]],\n",
       "         device='cuda:0'),\n",
       "  'input_ids': tensor([[49406,   320,  1125,   539,   320,  7615,   314, 16590, 10024,   318,\n",
       "            4298,   316, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0')})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义loader\n",
    "def collate_fn(data):\n",
    "    pixel_values = [i['pixel_values'] for i in data]\n",
    "    input_ids = [i['input_ids'] for i in data]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values).to(device)\n",
    "    input_ids = torch.stack(input_ids).to(device)\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     batch_size=1)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 1e-05\n",
       "     maximize: False\n",
       "     weight_decay: 0.01\n",
       " ),\n",
       " MSELoss())"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载模型\n",
    "from transformers import CLIPTextModel\n",
    "encoder=CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\",subfolder=\"text_encoder\")\n",
    "%run vae.ipynb\n",
    "%run unet.ipynb\n",
    "\n",
    "#准备训练\n",
    "encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "#only train the embedding layer\n",
    "encoder.text_model.embeddings.token_embedding.requires_grad_(True)\n",
    "\n",
    "encoder.eval()\n",
    "vae.eval()\n",
    "unet.train()\n",
    "\n",
    "encoder.to(device)\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(unet.parameters(),\n",
    "                              lr=1e-5,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              weight_decay=0.01,\n",
    "                              eps=1e-8)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_new_word():\n",
    "    #字典里添加新词\n",
    "    tokenizer.add_tokens('[snorlax_bear]')\n",
    "\n",
    "    #扩展encoder的embed层,添加一个新空间用于容纳新词\n",
    "    encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #取新旧两个词的id\n",
    "    old_id = tokenizer.convert_tokens_to_ids('Bear')\n",
    "    new_id = tokenizer.convert_tokens_to_ids('[snorlax_bear]')\n",
    "\n",
    "    embed = encoder.get_input_embeddings().weight.data\n",
    "\n",
    "    #以旧词来初始化新词\n",
    "    embed[new_id] = embed[old_id]\n",
    "\n",
    "\n",
    "init_new_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(data):\n",
    "    #[1, 77] -> [1, 77, 768]\n",
    "    out_encoder = encoder(data['input_ids'])[0]\n",
    "\n",
    "    #[1, 3, 512, 512] -> [1, 4, 64, 64]\n",
    " \n",
    "    out_vae = vae.encoder(data['pixel_values'])\n",
    "    out_vae = vae.sample(out_vae)\n",
    "\n",
    "    #0.18215 = vae.config.scaling_factor\n",
    "    out_vae = out_vae * 0.18215\n",
    "\n",
    "    #noise generator\n",
    "    noise = torch.randn_like(out_vae)\n",
    "\n",
    "    #add noise\n",
    "    #1000 = scheduler.num_train_timesteps\n",
    "    #1 = batch size\n",
    "    noise_step = torch.randint(0, 1000, (1, )).long().to(device)\n",
    "    out_vae_noise = scheduler.add_noise(out_vae, noise, noise_step)\n",
    "\n",
    "    #calc noise\n",
    "    out_unet = unet(out_vae=out_vae_noise,\n",
    "                    out_encoder=out_encoder,\n",
    "                    time=noise_step)\n",
    "\n",
    "    #mse loss\n",
    "    #[1, 4, 64, 64],[1, 4, 64, 64]\n",
    "    return criterion(out_unet, noise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:02<08:13,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6553820911794901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [00:14<03:52,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 6.480514218565077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:26<03:38,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 5.024804863380268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 31/200 [00:38<03:26,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 5.818547120783478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [00:51<03:13,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 5.353631235891953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 51/200 [01:03<03:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 6.0187197513878345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 61/200 [01:15<02:49,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 6.251606137491763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 71/200 [01:27<02:37,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 4.837037515128031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 81/200 [01:39<02:25,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 5.059074798366055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 91/200 [01:51<02:11,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 5.157424732111394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [02:04<01:59,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 4.843293134355918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 111/200 [02:16<01:48,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 5.771075590047985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 121/200 [02:28<01:35,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 6.247637171763927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 131/200 [02:40<01:23,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 6.22042535059154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 141/200 [02:52<01:11,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 5.106386477360502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 151/200 [03:04<00:59,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 5.224767256062478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 161/200 [03:16<00:47,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 5.168128897901624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 171/200 [03:28<00:35,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 4.84971322491765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 181/200 [03:40<00:22,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 4.157948060659692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 191/200 [03:53<00:10,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 4.7174648102372885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:03<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def train():\n",
    "    loss_sum = 0\n",
    "    for epoch in tqdm(range(200)):\n",
    "        for i, data in enumerate(loader):\n",
    "            loss = get_loss(data)  \n",
    "            #print(epoch, i, loss)\n",
    "            loss.backward()\n",
    "            loss_sum += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch, loss_sum)\n",
    "            loss_sum = 0\n",
    "\n",
    "    #torch.save(unet.to('cpu'), 'saves/unet.model')\n",
    "\n",
    "\n",
    "train()\n",
    "torch.save( encoder.to('cpu'), 'saves/encoder.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
